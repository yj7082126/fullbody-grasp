{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import smplx\n",
    "import meshplot as mp\n",
    "from psbody.mesh import Mesh\n",
    "from psbody.mesh.colors import name_to_rgb\n",
    "\n",
    "from flex.pretrained_models.loader_grabnet import Trainer as RHGrasp_Loader\n",
    "from flex.pretrained_models.loader_vposer import Trainer as VPoser_Loader\n",
    "from flex.tools.config import Config\n",
    "from flex.tools.utils import get_ground, replace_topk, aa2rotmat, rotmat2aa\n",
    "from run import Optimize\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES=6, python run.py \\\n",
    "# --obj_name stapler \\\n",
    "# --receptacle_name receptacle_aabb_TvStnd1_Top3_frl_apartment_tvstand \\\n",
    "# --ornt_name all \\\n",
    "# --gender 'female'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_name = \"stapler\"\n",
    "receptacle_name = \"receptacle_aabb_TvStnd1_Top3_frl_apartment_tvstand\"\n",
    "ornt_name = \"all\"\n",
    "index = 0\n",
    "bs = 1\n",
    "\n",
    "cfg_vp = OmegaConf.structured(Config)                   # Load base config\n",
    "cfg_yaml = OmegaConf.load('flex/configs/flex.yaml')    # Load yaml config\n",
    "cfg_rh = OmegaConf.load('flex/configs/rh.yaml')\n",
    "cfg_vp = OmegaConf.merge(cfg_vp, cfg_yaml)\n",
    "cfg_rh.batch_size = cfg_vp.bs = bs\n",
    "\n",
    "sbj_m = smplx.create(\n",
    "    model_path=cfg_vp.smplx_dir, model_type='smplx', gender='neutral',\n",
    "    num_pca_comps=24, batch_size=1\n",
    ")\n",
    "\n",
    "dset_info_dict = dict(np.load('data/replicagrasp/dset_info.npz', allow_pickle=1))\n",
    "transl_grab, orient_grab, recept_idx = dset_info_dict[f'{obj_name}_{receptacle_name}_{ornt_name}_{index}']\n",
    "obj_transl = torch.Tensor(transl_grab).to(device)\n",
    "obj_global_orient = rotmat2aa(torch.Tensor(orient_grab).reshape(1,1,1,9)).reshape(1,3).to(device)\n",
    "\n",
    "recept_dict = dict(np.load('data/replicagrasp/receptacles.npz', allow_pickle=1))\n",
    "obstacles_dict = {receptacle_name: recept_dict[receptacle_name][recept_idx]}\n",
    "# obstacles_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f1e37896de4a41ae73c8ccb3709caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.4612175…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_mesh = Mesh(filename=os.path.join(cfg_vp.obj_meshes_dir, obj_name + '.ply'), vscale=1.)\n",
    "obj_verts = np.matmul(object_mesh.v, orient_grab.T) + transl_grab\n",
    "mp_viewer = mp.plot(obj_verts, object_mesh.f, name_to_rgb['blue'])\n",
    "\n",
    "mp_viewer.add_mesh(sbj_m().vertices[0].detach().cpu().numpy(), sbj_m.faces, name_to_rgb['green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Trainable Parameters for Generative Hand Grasping Model (CoarseNet) is 14.04 M.\n",
      "Total Trainable Parameters for Generative Hand Grasping Model (RefineNet) is 3.26 M.\n",
      "------> Loading CoarseNet model (pre-trained on GRAB right-hand grasps) from /data/3D_dataset/hand_grasp/GrabNet/grabnet/models/coarsenet.pt\n",
      "------> Loading RefineNet model (pre-trained on GRAB right-hand grasps) from /data/3D_dataset/hand_grasp/GrabNet/grabnet/models/refinenet.pt\n",
      "\n",
      "\n",
      "Total Trainable Parameters for Human Generative Model (VPoser) is 0.94 M.\n",
      "------> Loading VPoser model (pre-trained on AMASS) from /data/3D_dataset/smpl_related/models/V02_05/snapshots/V02_05_epoch=13_val_loss=0.03.ckpt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tto = Optimize(cfg_rh=cfg_rh, cfg_vp=cfg_vp)\n",
    "_ = tto.mime_net.eval()\n",
    "_ = tto.coarse_net.eval()\n",
    "_ = tto.refine_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Trainable Parameters for Pose-Ground Generative Model (PGP) is 2.15 K.\n",
      "------> Loading PGP model (pre-trained on AMASS) from flex/pretrained_models/ckpts/pgp.pth\n",
      "\n",
      "Total Trainable Parameters for FLEX is 292922.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0f5105b35048a0a4f4f1a15d4a869c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TTO loss: 21.53\n",
      "Ending TTO loss: 0.04777\n",
      "Best   TTO loss: 0.04257\n"
     ]
    }
   ],
   "source": [
    "object_mesh = Mesh(filename=os.path.join(cfg_vp.obj_meshes_dir, obj_name + '.ply'), vscale=1.)\n",
    "object_mesh.reset_normals()\n",
    "object_mesh = [object_mesh.v, object_mesh.f]\n",
    "\n",
    "obj_bps, _, a_inits = tto.displace(obj_name, [obj_transl, obj_global_orient], 1, object_mesh)\n",
    "t_inits, g_inits, z_inits, w_inits = tto.get_inits([obj_transl, obj_global_orient], obj_bps, 1)\n",
    "\n",
    "curr_res = tto.perform_optim(z_inits, t_inits, g_inits, w_inits, a_inits,\n",
    "                                obstacles_dict, obj_bps, object_mesh,\n",
    "                                obj_transl, obj_global_orient, obj_name, 'latent')\n",
    "\n",
    "# (*) Save topk results.\n",
    "# results = replace_topk(curr_res, min(bs, 10))\n",
    "# final_result = {\n",
    "#     'human_vertices': results['human_vertices'][0].detach().cpu().numpy(),\n",
    "#     'pose': results['pose_final'][0].reshape(21, 3).detach().cpu().numpy(),\n",
    "#     'transl': results['transl_final'][0].detach().cpu().numpy(),\n",
    "#     'global_orient': aa2rotmat(results['global_orient_final'])[0].view(3, 3).detach().cpu().numpy(),\n",
    "#     'rh_verts': results['rh_verts'][0].detach().cpu().numpy(),\n",
    "#     'z_final': results['z_final'][0].detach().cpu().numpy(),\n",
    "# }\n",
    "body_params = {\n",
    "    'transl': curr_res['transl_final'][0:1],\n",
    "    'global_orient': curr_res['global_orient_final'][0:1],\n",
    "    'body_pose': curr_res['pose_final'][0:1]\n",
    "}\n",
    "body_vertices = sbj_m(**body_params).vertices.detach().cpu().numpy()[0]\n",
    "body_vertices[..., 2] -= body_vertices[..., 2].min(-1, keepdims=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8c6b078c69476180fedb93d4b11223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.4612175…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_mesh = Mesh(filename=os.path.join(cfg_vp.obj_meshes_dir, obj_name + '.ply'), vscale=1.)\n",
    "obj_verts = np.matmul(object_mesh.v, orient_grab.T) + transl_grab\n",
    "mp_viewer = mp.plot(obj_verts, object_mesh.f, name_to_rgb['blue'])\n",
    "\n",
    "# mp_viewer.add_mesh(\n",
    "#     obstacles_dict[receptacle_name][0], \n",
    "#     obstacles_dict[receptacle_name][1], \n",
    "#     name_to_rgb['yellow']\n",
    "# )\n",
    "\n",
    "# grnd_mesh = get_ground(grnd_size=10, offset=0.0)\n",
    "# xmean, ymean, _ = obstacles_dict[receptacle_name][0].mean(0)\n",
    "# grnd_mesh.v[:, 0] += xmean\n",
    "# grnd_mesh.v[:, 1] += ymean\n",
    "# mp_viewer.add_mesh(grnd_mesh.v, grnd_mesh.f, name_to_rgb['brown'])\n",
    "\n",
    "mp_viewer.add_mesh(body_vertices, sbj_m.faces, name_to_rgb['green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
