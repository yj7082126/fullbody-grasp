{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"\n",
    "\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import open3d as o3d\n",
    "import smplx\n",
    "from smplx.lbs import batch_rodrigues\n",
    "import trimesh\n",
    "import torch\n",
    "import pyrender\n",
    "import meshplot as mp\n",
    "\n",
    "from utils.cfg_parser import Config\n",
    "from utils.utils import makelogger, makepath\n",
    "from WholeGraspPose.models.fittingop import FittingOP\n",
    "from WholeGraspPose.trainer import Trainer\n",
    "from utils.utils import RotConverter\n",
    "# from WholeGraspPose.models.objectmodel import ObjectModel\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/data/3D_dataset/GOAL/\")\n",
    "from tools.objectmodel import ObjectModel\n",
    "from tools.utils import to_cpu, to_tensor, np2torch, euler\n",
    "from tools.meshviewer import Mesh\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 06:52:59,433 - root - INFO - Using 1 CUDA cores [NVIDIA A100 80GB PCIe] for training!\n",
      "2024-09-10 06:52:59,437 - root - INFO - {'dataset_dir': '/data/3D_dataset/GrabNet/data/GRAB/data/FullGraspPose', 'work_dir': 'results/pretrained_male_tmp/GraspPose', 'vpe_path': 'configs/verts_per_edge.npy', 'c_weights_path': 'WholeGraspPose/configs/rhand_weight.npy', 'exp_name': 'pretrained_male_tmp', 'gender': 'male', 'best_net': 'pretrained_model/male_grasppose_model.pt', 'base_lr': 0.0005, 'batch_size': 128, 'bps_size': 4096, 'cuda_id': 0, 'kl_coef': 0.005, 'latentD': 16, 'log_every_epoch': 10, 'n_epochs': 100, 'n_markers': 512, 'n_neurons': 512, 'n_workers': 8, 'reg_coef': 0.0005, 'seed': 4815, 'try_num': 0, 'use_multigpu': False, 'load_on_ram': False, 'cond_object_height': True, 'motion_intent': False, 'object_class': ['all'], 'robustkl': False, 'kl_annealing': True, 'kl_annealing_epoch': 100, 'marker_weight': 1, 'foot_weight': 0, 'collision_weight': 0, 'consistency_weight': 1, 'dropout': 0.1, 'obj_feature': 12, 'pointnet_hc': 64, 'continue_train': False, 'data_representation': 'markers_143', 'default_cfg': {'base_lr': 0.0005, 'batch_size': 128, 'best_net': 'pretrained_model/male_grasppose_model.pt', 'bps_size': 4096, 'c_weights_path': 'WholeGraspPose/configs/rhand_weight.npy', 'cuda_id': 0, 'dataset_dir': '/data/3D_dataset/GrabNet/data/GRAB/data/FullGraspPose', 'kl_coef': 0.005, 'latentD': 16, 'log_every_epoch': 10, 'n_epochs': 100, 'n_markers': 512, 'n_neurons': 512, 'n_workers': 8, 'reg_coef': 0.0005, 'seed': 4815, 'try_num': 0, 'use_multigpu': False, 'vpe_path': 'configs/verts_per_edge.npy', 'work_dir': 'results/pretrained_male_tmp/GraspPose', 'load_on_ram': False, 'cond_object_height': True, 'motion_intent': False, 'object_class': ['all'], 'robustkl': False, 'kl_annealing': True, 'kl_annealing_epoch': 100, 'marker_weight': 1, 'foot_weight': 0, 'collision_weight': 0, 'consistency_weight': 1, 'dropout': 0.1, 'obj_feature': 12, 'pointnet_hc': 64, 'continue_train': False, 'data_representation': 'markers_143', 'exp_name': 'pretrained_male_tmp', 'gender': 'male'}}\n",
      "2024-09-10 06:53:02,260 - root - INFO - Total Trainable Parameters for ContactNet is 7.64 M.\n",
      "2024-09-10 06:53:02,339 - root - INFO - Restored ContactNet model from pretrained_model/male_grasppose_model.pt\n"
     ]
    }
   ],
   "source": [
    "cfg = Config(\n",
    "    default_cfg_path='WholeGraspPose/configs/WholeGraspPose.yaml', \n",
    "    dataset_dir='/data/3D_dataset/GrabNet/data/GRAB/data/FullGraspPose',\n",
    "    work_dir=\"results/pretrained_male_tmp/GraspPose\",\n",
    "    vpe_path=\"configs/verts_per_edge.npy\",\n",
    "    c_weights_path=\"WholeGraspPose/configs/rhand_weight.npy\",\n",
    "    exp_name=\"pretrained_male_tmp\",\n",
    "    gender=\"male\",\n",
    "    best_net=\"pretrained_model/male_grasppose_model.pt\"\n",
    ")\n",
    "save_dir = f\"results/pretrained_male_tmp/GraspPose/tmp\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "logger = makelogger(makepath(os.path.join(save_dir, f'tmp.log'), isfile=True)).info\n",
    "grabpose = Trainer(cfg=cfg, inference=True, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-0bfe87685fb1>:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  global_orient = torch.tensor([np.deg2rad(rot)]).to(dtype=torch.float32, device=device)\n",
      "2024-09-10 06:53:07.415 | INFO     | human_body_prior.tools.model_loader:load_model:97 - Loaded model in eval mode with trained weights: /data/3D_dataset/smpl_related/models/V02_05/snapshots/V02_05_epoch=13_val_loss=0.03.ckpt\n"
     ]
    }
   ],
   "source": [
    "obj_name = '/data/3D_dataset/GrabNet/tools/object_meshes/contact_meshes/camera.ply'\n",
    "trans = [0.0, 0.0, 0.9] # -1: torch.rand(1) + 0.6\n",
    "rot = [0.0, 0.0, 90.0] # -1: (np.pi)*torch.rand(1) - np.pi/2 \n",
    "\n",
    "obj_mesh_base = o3d.io.read_triangle_mesh(obj_name)\n",
    "obj_mesh_base.compute_vertex_normals()\n",
    "v_temp = torch.from_numpy(np.expand_dims(np.asarray(obj_mesh_base.vertices), 0)).to(dtype=torch.float32, device=device)\n",
    "normal_temp = torch.from_numpy(np.expand_dims(np.asarray(obj_mesh_base.vertex_normals), 0)).to(dtype=torch.float32, device=device)\n",
    "\n",
    "transf_transl = torch.tensor([trans]).to(dtype=torch.float32, device=device)\n",
    "global_orient = torch.tensor([np.deg2rad(rot)]).to(dtype=torch.float32, device=device)\n",
    "global_orient_rotmat = batch_rodrigues(global_orient.view(-1, 3)).to(device)   # [N, 3, 3]\n",
    "global_orient_rotmat_6d = global_orient_rotmat.view(-1, 1, 9)[:, :, :6]\n",
    "\n",
    "object_verts = torch.matmul(v_temp, global_orient_rotmat)\n",
    "object_normal = torch.matmul(normal_temp, global_orient_rotmat)\n",
    "\n",
    "index = np.linspace(0, object_verts.shape[1], num=2048, endpoint=False,retstep=True,dtype=int)[0]\n",
    "verts_object, normal_object = object_verts[:, index], object_normal[:, index]\n",
    "feat_object = torch.cat([normal_object, global_orient_rotmat_6d.repeat(1,2048,1)], dim=-1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    markers_gen, markers_contact_gen, object_contact_gen = grabpose.full_grasp_net.sample(\n",
    "        verts_object.permute(0,2,1), \n",
    "        feat_object.permute(0,2,1), \n",
    "        transf_transl\n",
    "    )\n",
    "    markers_gen += transf_transl\n",
    "    verts_object += transf_transl\n",
    "\n",
    "fittingop = FittingOP({ \n",
    "    'init_lr_h': 0.008, 'num_iter': [300,400,500], 'batch_size': 1,\n",
    "    'num_markers': 143, 'device': device, 'cfg': cfg,\n",
    "    'verbose': False, 'hand_ncomps': 24, 'only_rec': False,     # True / False \n",
    "    'contact_loss': 'contact', 'logger': logger,\n",
    "    'data_type': 'markers_143',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4094830d0d624f16956b49a257ff2fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.0040337â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/icon/lib/python3.8/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"uint32\" does not match required type \"float64\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<meshplot.Viewer.Viewer at 0x7fb24f70cb20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_mesh = Mesh(filename=obj_name, vscale=1.0)\n",
    "object_verts = torch.matmul(v_temp, global_orient_rotmat) + transf_transl.unsqueeze(dim=1)\n",
    "# obj_i = Mesh(vertices=object_verts[0].detach().cpu().numpy(), faces=obj_mesh.faces, vc=[1.0,0.0,0.0])\n",
    "\n",
    "fittingop.reset()\n",
    "smplx_results_0 = fittingop.bm_male(\n",
    "    transl=fittingop.transl_rec, \n",
    "    global_orient=RotConverter.rotmat2aa(RotConverter.cont2rotmat(fittingop.glo_rot_rec)),\n",
    "    body_pose=fittingop.vposer.decode(fittingop.vpose_rec).get('pose_body'),\n",
    "    return_verts=True\n",
    ")\n",
    "# sbj_i = Mesh(vertices=smplx_results.vertices[0].detach().cpu().numpy(), faces=fittingop.bm_male.faces, vc=[0.5,0.5,0.5])\n",
    "\n",
    "mp_viewer = mp.plot(smplx_results_0.vertices[0].detach().cpu().numpy(), fittingop.bm_male.faces, np.array([0.75,0.75,0.75]))\n",
    "mp_viewer.add_mesh(object_verts[0].detach().cpu().numpy(), obj_mesh.faces, np.array([1.0,0.0,0.0]))\n",
    "mp_viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 06:56:00,720 - root - INFO - Save [stage2] iter=445, loss:total: 0.0176 |  rec: 0.0065 |  body_rec: 0.0064 |  marker contact: 0.0002 |  object contact: 0.0004 |  prior contact: 0.0009 |  hand collision: 0.0001 |  object collision: 0.0001 |  foot: 0.0004 |  reg: 0.0002 | , verts_info:hand colli: 4 |  obj colli: 6 |  contact: 3 |  hand markers colli: 0 | \n"
     ]
    }
   ],
   "source": [
    "fittingop.reset()\n",
    "markers_fit_gen, smplxparams_gen, loss_gen = fittingop.fitting(\n",
    "    markers_gen, object_contact_gen, markers_contact_gen, \n",
    "    verts_object, normal_object, 'male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0954027223ed4c1bbbc4aa38e814385f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.0231063â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<meshplot.Viewer.Viewer at 0x7fb24f75e9d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_mesh = Mesh(filename=obj_name, vscale=1.0)\n",
    "object_verts = torch.matmul(v_temp, global_orient_rotmat) + transf_transl.unsqueeze(dim=1)\n",
    "# obj_i = Mesh(vertices=object_verts[0].detach().cpu().numpy(), faces=obj_mesh.faces, vc=[1.0,0.0,0.0])\n",
    "\n",
    "fittingop.reset()\n",
    "smplx_results = fittingop.bm_male(\n",
    "    return_verts=True,\n",
    "    **smplxparams_gen[-1]\n",
    ")\n",
    "# sbj_i = Mesh(vertices=smplx_results.vertices[0].detach().cpu().numpy(), faces=fittingop.bm_male.faces, vc=[0.5,0.5,0.5])\n",
    "\n",
    "mp_viewer = mp.plot(smplx_results.vertices[0].detach().cpu().numpy(), fittingop.bm_male.faces, np.array([0.75,0.75,0.75]))\n",
    "# mp_viewer.add_mesh(smplx_results_0.vertices[0].detach().cpu().numpy(), fittingop.bm_male.faces, np.array([0.25,0.4,0.8]))\n",
    "mp_viewer.add_mesh(object_verts[0].detach().cpu().numpy(), obj_mesh.faces, np.array([1.0,0.0,0.0]))\n",
    "mp_viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_mesh = Mesh(filename=obj_name, vscale=1.0)\n",
    "object_verts = torch.matmul(v_temp, global_orient_rotmat) + transf_transl.unsqueeze(dim=1)\n",
    "obj_i = Mesh(vertices=object_verts[0].detach().cpu().numpy(), faces=obj_mesh.faces, vc=[1.0,0.0,0.0])\n",
    "\n",
    "smplx_results = fittingop.bm_male(return_verts=True, **smplxparams_gen[-1])\n",
    "sbj_i = Mesh(vertices=smplx_results.vertices[0].detach().cpu().numpy(), faces=fittingop.bm_male.faces, vc=[0.5,0.5,0.5])\n",
    "\n",
    "obj_array = [obj_i, sbj_i]\n",
    "\n",
    "angle = [-90, 0, 0]\n",
    "trans = [0.0, -1.0, 0.0]\n",
    "z_dist = 2.0\n",
    "a_light = 0.4\n",
    "d_light = 3.0\n",
    "w, h = 512, 512\n",
    "\n",
    "scene = pyrender.Scene(bg_color=[0.0,0.0,0.0,1.0], ambient_light=a_light, name='scene')\n",
    "\n",
    "pc = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1.0)\n",
    "camera_pose = np.eye(4)\n",
    "camera_pose[:3, :3] = euler([0.0, 0.0, 0.0], 'xzy')\n",
    "camera_pose[:3, 3] = np.array([0.0, 0.0, z_dist])\n",
    "cam = pyrender.Node(name = 'camera', camera=pc, matrix=camera_pose)\n",
    "scene.add_node(cam)\n",
    "\n",
    "light = pyrender.light.DirectionalLight(color=np.ones(3), intensity=d_light)\n",
    "light = pyrender.Node(light=light, matrix=camera_pose)\n",
    "scene.add_node(light)\n",
    "\n",
    "for obj in obj_array:\n",
    "    obj.rot_verts(euler(angle, 'xzy'))\n",
    "    obj.vertices += trans\n",
    "    mesh = pyrender.Mesh.from_trimesh(obj)\n",
    "    scene.add(mesh)\n",
    "\n",
    "viewer = pyrender.OffscreenRenderer(w, h)\n",
    "color, depth_buffer = viewer.render(scene)\n",
    "viewer.delete()\n",
    "\n",
    "depth = depth_buffer.copy()\n",
    "mask = depth > 0\n",
    "color_image = Image.fromarray(np.concatenate([color, (mask[...,np.newaxis]*255.).astype(np.uint8)], axis=-1))\n",
    "color_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vis Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_path = Path(\"results/pretrained_male_tmp2/GraspPose/bowl/fitting_results.npz\")\n",
    "# data = np.load(load_path, allow_pickle=True)\n",
    "# index = 0\n",
    "# # object_mesh = get_object_mesh(object_name, 'GRAB', data['object'][()]['transl'][:n_samples], data['object'][()]['global_orient'][:n_samples], n_samples)\n",
    "\n",
    "# obj_path = '/data/3D_dataset/GrabNet/data/GRAB/data/tools/object_meshes/contact_meshes/camera.ply'\n",
    "# transl = torch.tensor(data['object'][()]['transl'][index], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "# global_orient = torch.tensor(data['object'][()]['global_orient'][index], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "# obj_mesh = Mesh(filename=obj_path, vscale=1.0)\n",
    "# obj_m = ObjectModel(v_template=torch.from_numpy(obj_mesh.vertices)).to(device)\n",
    "# obj_m.faces = obj_mesh.faces\n",
    "# verts_obj = obj_m(transl=transl, global_orient=global_orient).vertices[0].cpu().numpy()\n",
    "# obj_i = Mesh(vertices=verts_obj, faces=obj_mesh.faces, vc=[1.0,0.0,0.0])\n",
    "\n",
    "# # body_mesh, _ = get_body_mesh(data['body'][()], gender, n_samples)\n",
    "\n",
    "# body_model = smplx.create(\n",
    "#     '/data/3D_dataset/smpl_related/models', \n",
    "#     model_type='smplx', gender='male', ext='npz',\n",
    "#     num_pca_comps=24, batch_size=1, v_template=None\n",
    "# ).to(device)\n",
    "\n",
    "# smplxparams = data['body'][()]\n",
    "# for key in smplxparams.keys():\n",
    "#     smplxparams[key] = torch.tensor(smplxparams[key][index], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "# smplx_results = body_model(return_verts=True, **smplxparams)\n",
    "# verts = smplx_results.vertices[0].detach().cpu().numpy()\n",
    "# face = body_model.faces\n",
    "# sbj_i = Mesh(vertices=verts, faces=face, vc=[0.5,0.5,0.5])\n",
    "\n",
    "# angle = [-90, 0, 0]\n",
    "# trans = [0.0, -1.0, 0.0]\n",
    "# z_dist = 1.0\n",
    "# a_light = 0.4\n",
    "# d_light = 3.0\n",
    "# w, h = 512, 512\n",
    "\n",
    "# obj_array = [obj_i, sbj_i]\n",
    "\n",
    "# scene = pyrender.Scene(bg_color=[0.0,0.0,0.0,1.0], ambient_light=a_light, name='scene')\n",
    "\n",
    "# pc = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1.0)\n",
    "# camera_pose = np.eye(4)\n",
    "# camera_pose[:3, :3] = euler([0.0, 0.0, 0.0], 'xzy')\n",
    "# camera_pose[:3, 3] = np.array([0.0, 0.0, z_dist])\n",
    "# cam = pyrender.Node(name = 'camera', camera=pc, matrix=camera_pose)\n",
    "# scene.add_node(cam)\n",
    "\n",
    "# light = pyrender.light.DirectionalLight(color=np.ones(3), intensity=d_light)\n",
    "# light = pyrender.Node(light=light, matrix=camera_pose)\n",
    "# scene.add_node(light)\n",
    "\n",
    "# for obj in obj_array:\n",
    "#     obj.rot_verts(euler(angle, 'xzy'))\n",
    "#     obj.vertices += trans\n",
    "#     mesh = pyrender.Mesh.from_trimesh(obj)\n",
    "#     scene.add(mesh)\n",
    "\n",
    "# viewer = pyrender.OffscreenRenderer(w, h)\n",
    "# color, depth_buffer = viewer.render(scene)\n",
    "# viewer.delete()\n",
    "\n",
    "# depth = depth_buffer.copy()\n",
    "# mask = depth > 0\n",
    "# color_image = Image.fromarray(np.concatenate([color, (mask[...,np.newaxis]*255.).astype(np.uint8)], axis=-1))\n",
    "# color_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
